#!/usr/local/bin/perl -w

use strict;
use Carp;
use FileHandle;

##########################################################
##  VECTOR1
##
##  Usage:   vector1     (no command line arguments)
##
##  The function &main_loop below gives the menu for the system.
##
##  This is an example program that shows how the core
##  of a vector-based IR engine may be implemented in Perl.
##
##  Some of the functions below are unimplemented, and some
##  are only partially implemented. Suggestions for additions
##  are given below and in the assignment handout.
##
##  You should feel free to modify this program directly,
##  and probably use this as a base for your implemented
##  extensions.  As with all assignments, the range of
##  possible enhancements is open ended and creativity
##  is strongly encouraged.
##########################################################


############################################################
## Program Defaults and Global Variables
############################################################

##my $DIR  = "/home/1/yarowsky/cs466/hw2";
# changed directory so that it can run on my local machine
my $DIR = ".";
my $HOME = ".";

my $token_docs = "$DIR/myfile";           # tokenized cacm journals
my $corps_freq = "$DIR/myfile";           # frequency of each token in the journ.
my $stoplist   = "$DIR/common_words";   # common uninteresting words


# @doc_vector
#
#   An array of hashes, each array index indicating a particular document's
#   weight "vector".

my @doc_vector = ( );


# %docs_freq_hash
#
# associative array which holds <token, frequency> pairs where
#
#   token     = a particular word or tag found in the cacm corpus
#   frequency = the total number of times the token appears in
#               the corpus.

my %docs_freq_hash = ( );

# %corp_freq_hash
#
# associative array which holds <token, frequency> pairs where
#
#   token     = a particular word or tag found in the corpus
#   frequency = the total number of times the token appears per
#               document-- that is a token is counted only once
#               per document if it is present (even if it appears
#               several times within that document).

my %corp_freq_hash = ( );

# %stoplist_hash
#
# common list of uninteresting words which are likely irrelvant
# to any query.
#
#   Note: this is an associative array to provide fast lookups
#         of these boring words

my %stoplist_hash  = ( );


# %relevance_hash
#
# a hash of hashes where each <key, value> pair consists of
#
#   key   = a query number
#   value = a hash consisting of document number keys with associated
#           numeric values indicating the degree of relevance the
#           document has to the particular query.

my %relevance_hash = ( );

# @doc_simula
#
# array used for storing query to document or document to document
# similarity calculations (determined by cosine_similarity, etc. )

my @doc_simula = ( );

# @res_vector
#
# array used for storing the document numbers of the most relevant
# documents in a query to document or document to document calculation.

my @res_vector = ( );


# start program

&main_loop;

##########################################################
##  INIT_FILES
##
##  This function specifies the names and locations of
##  input files used by the program.
##
##  Parameter:  $type   ("stemmed" or "unstemmed")
##
##  If $type == "stemmed", the filenames are initialized
##  to the versions stemmed with the Porter stemmer, while
##  in the default ("unstemmed") case initializes to files
##  containing raw, unstemmed tokens.
##########################################################

sub init_files {

    if ("stemmed" eq (shift || "")) {

	$token_docs .= "\.stemmed";
	$corps_freq .= "\.stemmed\.hist";
	$stoplist   .= "\.stemmed";

    }
    else {

	$token_docs .= "\.tokenized";
	$corps_freq .= "\.tokenized\.hist";


    }
}

##########################################################
##  INIT_CORP_FREQ
##
##  This function reads in corpus and document frequencies from
##  the provided histogram file for both the document set
##  and the query set. This information will be used in
##  term weighting.
##
##  It also initializes the arrays representing the stoplist,
##  title list and relevance of document given query.
##########################################################

sub init_corp_freq {

    my $corps_freq_fh = new FileHandle $corps_freq, "r"
	or croak "Failed $corps_freq";

    my $stoplist_fh   = new FileHandle $stoplist  , "r"
	or croak "Failed $stoplist";

    my $line = undef;

    while (defined( $line = <$corps_freq_fh> )) {

	# so on my computer split will return a first element of undef
	# if the leading characters are white space, so I eat the white
	# space to insure that the split works right.

	my ($str) = ($line =~ /^\s*(\S.*)/);

	my ($doc_freq,
	    $cor_freq,
	    $term    ) = split /\s+/, $str;

	$docs_freq_hash{ $term } = $doc_freq;
	$corp_freq_hash{ $term } = $cor_freq;
    }

    while (defined( $line = <$stoplist_fh> )) {

	chomp $line;
	$stoplist_hash{ $line } = 1;
    }

}


##########################################################
##  INIT_DOC_VECTORS
##
##  This function reads in tokens from the document file.
##  When a .I token is encountered, indicating a document
##  break, a new vector is begun. When individual terms
##  are encountered, they are added to a running sum of
##  term frequencies. To save time and space, it is possible
##  to normalize these term frequencies by inverse document
##  frequency (or whatever other weighting strategy is
##  being used) while the terms are being summed or in
##  a posthoc pass.  The 2D vector array
##
##    $doc_vector[ $doc_num ]{ $term }
##
##  stores these normalized term weights.
##
##  It is possible to weight different regions of the document
##  differently depending on likely importance to the classification.
##  The relative base weighting factors can be set when
##  different segment boundaries are encountered.
##
##  This function is currently set up for simple TF weighting.
##########################################################

sub init_doc_vectors {
    my $token_docs_fh = new FileHandle $token_docs, "r"
	or croak "Failed $token_docs";

    my $word    = undef;

    my $doc_num =  0;    # current document number and total docs at end
    my $tweight =  0;    # current weight assigned to document token

    push @doc_vector, { };     # push one empty value onto @doc_vector so that
                               # indices correspond with document numbers

    while (defined( $word = <$token_docs_fh> )) {

	chomp $word;

	last if $word =~ /^\.I 0$/; # indicates end of file so kick out

	if ($word =~ /^\.I/) {     # indicates start of a new document

	    push @doc_vector, { };
	    $doc_num++;
      if($doc_num == 56) {
        print $word;
      }

	    next;
	}


	# not implemented yet
	if ($word =~ /[a-zA-Z]/ and ! exists $stoplist_hash{ $word }) {

  #    print $word, "\n";
  #    print $docs_freq_hash{ $word }, "\n";

	    if (defined( $docs_freq_hash{ $word } )) {

				$doc_vector[$doc_num]{ $word } += 1; # Term Frequency: # occurences of term t in document d
  #      print $doc_vector[$doc_num]{ $word }, "\n";
	    }
	    else {
		print "ERROR: Document frequency of zero: ", $word, "\n";
	    }
	}
    }

    # optionally n(ormalize the raw term frequency
        # TF IDF weighting (wt_{t,d} = TF_{td} * log(N/DF)))

    	foreach my $hash (@doc_vector) {
    	  	foreach my $key (keys %{ $hash }) {
    	        $hash->{ $key } = $hash->{ $key } * log( $doc_num / $docs_freq_hash{ $key });
    		}
   		}


    return $doc_num;
}


##########################################################
## MAIN_LOOP
##
## Parameters: currently no explicit parameters.
##             performance dictated by user imput.
##
## Initializes document and query vectors using the
## input files specified in &init_files. Then offers
## a menu and switch to appropriate functions in an
## endless loop.
##
## Possible extensions at this level:  prompt the user
## to specify additional system parameters, such as the
## similarity function to be used.
##
## Currently, the key parameters to the system (stemmed/unstemmed,
## stoplist/no-stoplist, term weighting functions, vector
## similarity functions) are hardwired in.
##
## Initializing the document vectors is clearly the
## most time consuming section of the program, as 213334
## to 258429 tokens must be processed, weighted and added
## to dynamically growing vectors.
##
##########################################################

sub main_loop {
  print "INITIALIZING VECTORS ... \n";

  &init_files ("unstemmed");

	&init_corp_freq;

  my $total_docs = &init_doc_vectors();


  while (1) {

	print <<"EndOfMenu";

	============================================================
	==     Welcome to the 600.466 Cryptocurrency Analysis Engine
	==
        == Total Documents: $total_docs
	============================================================

	OPTIONS:
	  1 = Find the similarity between two cryptocurrencies
	  2 = Clustering cryptocurrencies and find competitors
	  3 = Find the best cryptocurrency for each clustering
	  4 = Quit

	============================================================

EndOfMenu
    ;

	print "Enter Option: ";

	my    $option = <STDIN>;
	chomp $option;

	exit 0 if $option == 4;

	&do_full_cosine_similarity and next if $option == 1;
#  &find_competitors and next if $option == 2;
#	&find_best and next if $option == 3;

    }
}



########################################################
## DO_FULL_COSINE_SIMILARITY
##
##  Prompts for a document number and query number,
##  and then calls a function to show similarity.
##
##  Could/should be expanded to handle a variety of
##  similarity measures.
########################################################

sub do_full_cosine_similarity {

    print "\n";
    print "1st Document number: ";

    my    $num_one = <STDIN>;
    chomp $num_one;

    print "\n";
    print "2nd Document number: ";

    my    $num_two = <STDIN>;
    chomp $num_two;

    $num_one = 1 if $num_one !~ /[0-9]/;
    $num_two = 1 if $num_two !~ /[0-9]/;

    full_cosine_similarity( $doc_vector[$num_one],
			    $doc_vector[$num_two],
			    $num_one,
			    $num_two );
}


########################################################
## FULL_COSINE_SIMILARITY
##
## UNIMPLEMENTED
##
## This function should compute cosine similarity between
## two vectors and display the information that went into
## this calculation, useful for debugging purposes.
## Similar in structure to &show_overlap.
########################################################

sub full_cosine_similarity {

    my $qry_vect = shift;
    my $doc_vect = shift;
    my $qry_indx = shift;
    my $doc_indx = shift;
    my $term_one   = undef;
    my $weight_one = undef;
    my $term_two   = undef;
    my $weight_two = undef;


    print "============================================================\n";
    printf( "%-15s  %8d   %8d\t%-15s\n",
	   "Vector Overlap",
	   $qry_indx        ,
	   $doc_indx        ,
	   "Product result"       );
    print "============================================================\n";


    my $num = 0;
    my $sum1 = 0;
    my $sum2 = 0;
    my $sum_square1 = 0;
    my $sum_square2 = 0;

    while (($term_one, $weight_one) = each %{ $qry_vect }) {
      #print $term_one, $weight_one, "\n";
    	$num += $weight_one * ($$doc_vect{ $term_one } || 0);
    	$sum1 += $weight_one;
    	$sum_square1 += ($weight_one * $weight_one);


    	if (exists $$doc_vect{ $term_one }) {
    		printf( "%-15s  %8d   %8d\t%8d\n",
    		$term_one                    ,
		   	$weight_one                  ,
		   	$$doc_vect{ $term_one }      ,
		   	$weight_one * $$doc_vect{ $term_one } );
    	}
    }
    # "sum_square1", $sum_square1, "\n";

    while (($term_two, $weight_two) = each %{ $doc_vect }) {
    #  print $term_two, $weight_two, "\n";
    	$sum2 += $weight_two;
    	$sum_square2 += ($weight_two * $weight_two);
    }

    #print "sum_square2", $sum_square2, "\n";

    my $cos_sim = &cosine_sim_b($num, $sum_square1, $sum_square2);

    print "==================================\n";
    printf( "%-15s\t\n", "Cosine Similarity");
	printf( "%8f\t\n", $cos_sim);
    print "==================================\n";
}


########################################################
## COSINE_SIM_A
##
## Computes the cosine similarity for two vectors
## represented as associate arrays.
########################################################

sub cosine_sim_a {

    my $vec1 = shift;
    my $vec2 = shift;

    my $num     = 0;
    my $sum_sq1 = 0;
    my $sum_sq2 = 0;

    my @val1 = values %{ $vec1 };
    my @val2 = values %{ $vec2 };

    # determine shortest length vector. This should speed
    # things up if one vector is considerable longer than
    # the other (i.e. query vector to document vector).

    if ((scalar @val1) > (scalar @val2)) {
	my $tmp  = $vec1;
	   $vec1 = $vec2;
	   $vec2 = $tmp;
    }

    # calculate the cross product

    my $key = undef;
    my $val = undef;

    while (($key, $val) = each %{ $vec1 }) {
	$num += $val * ($$vec2{ $key } || 0);
    }

    # calculate the sum of squares

    my $term = undef;

    foreach $term (@val1) { $sum_sq1 += $term * $term; }
    foreach $term (@val2) { $sum_sq2 += $term * $term; }

    return ( $num / sqrt( $sum_sq1 * $sum_sq2 ));
}


########################################################
##  COSINE_SIM_B
##
##  This function assumes that the sum of the squares
##  of the term weights have been stored in advance for
##  each document and are passed as arguments.
########################################################

sub cosine_sim_b {

    my $value = shift;
    my $sum_sq1 = shift;
    my $sum_sq2 = shift;

    return ( $value / sqrt( $sum_sq1 * $sum_sq2 ));
}
################################
## JACCARD_SIM_A
##
## Computes the cosine similarity for two vectors
## represented as associate arrays.
##
################################
sub jaccard_sim_a {

    my $vec1 = shift;
    my $vec2 = shift;

    my $num     = 0;
    my $sum_term_1 = 0;
    my $sum_term_2 = 0;

    my @val1 = values %{ $vec1 };
    my @val2 = values %{ $vec2 };

    # determine shortest length vector. This should speed
    # things up if one vector is considerable longer than
    # the other (i.e. query vector to document vector).

    if ((scalar @val1) > (scalar @val2)) {
	my $tmp  = $vec1;
	   $vec1 = $vec2;
	   $vec2 = $tmp;
    }

    # calculate the cross product

    my $key = undef;
    my $val = undef;

    while (($key, $val) = each %{ $vec1 }) {
	$num += $val * ($$vec2{ $key } || 0);
    }

    # calculate the sum of squares

    my $term = undef;

    foreach $term (@val1) { $sum_term_1 += $term; }
    foreach $term (@val2) { $sum_term_2 += $term; }

    return ( $num / (($sum_term_1 + $sum_term_2 ) - $num));

}

######################################################
##  JACCARD_SIM_B
##  This function assumes that the sum
##  of the term weights have been stored in advance for
##  each document and are passed as arguments.
##
######################################################

sub jaccard_sim_b {

    my $num  = shift;
    my $sum_term1 = shift;
    my $sum_term2 = shift;

    return ( $num / ($sum_term1 + $sum_term2 - $num) );
}

######################################################
##  DICE_SIM_A
##
##  Computes the dice similarity for two vectors
##  represented as associate arrays
##
######################################################

sub dice_sim_a {


    my $vec1 = shift;
    my $vec2 = shift;

    my $num     = 0;
    my $sum_term_1 = 0;
    my $sum_term_2 = 0;

    my @val1 = values %{ $vec1 };
    my @val2 = values %{ $vec2 };

    # determine shortest length vector. This should speed
    # things up if one vector is considerable longer than
    # the other (i.e. query vector to document vector).

    if ((scalar @val1) > (scalar @val2)) {
	my $tmp  = $vec1;
	   $vec1 = $vec2;
	   $vec2 = $tmp;
    }

    # calculate the cross product

    my $key = undef;
    my $val = undef;

    while (($key, $val) = each %{ $vec1 }) {
	$num += $val * ($$vec2{ $key } || 0);
    }

    # calculate the sum of squares

    my $term = undef;

    # 2 * numerator / (sum_term + sum_term)
    foreach $term (@val1) { $sum_term_1 += $term; }
    foreach $term (@val2) { $sum_term_2 += $term; }

    return ( 2 * $num / ($sum_term_1 + $sum_term_2));

}

######################################################
##  DICE_SIM_B
##  This function assumes that the sum
##  of the term weights have been stored in advance for
##  each document and are passed as arguments.
##
######################################################

sub dice_sim_b {


    my $num  = shift;
    my $sum_term1 = shift;
    my $sum_term2 = shift;

    return ( 2 * $num / ($sum_term1 + $sum_term2) );
}
